{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3608e28",
   "metadata": {},
   "source": [
    "# Домашнее задание №15. Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5ec029",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "1. Самостоятельно выбранными средствами (opencv, pillow (PIL), …) сгенерировать по 820 картинок размером 100х100 пикселей (px) для каждой из цифр: 0, 1, 3, 8 следующим образом (800 – тренировочная выборка, 20 – тестовая выборка № 1):\n",
    "* фон картинки белый,\n",
    "* цифра: ширина – 20 px, высота – 50 px, цвет линии – черный, цифра целиком помещается в картинку, цифра находится в случайном месте на картинке, \n",
    "* на изображении цифра расположена так, что ее вертикальная ось параллельна оси ординат (вертикальное положение) или оси абсцисс (горизонтальное положение),\n",
    "* тренировочная выборка содержит 400 изображений каждой цифры в горизонтальном положении и 400 изображений каждой цифры в вертикальном положении,\n",
    "* тестовая выборка содержит 10 изображений каждой цифры в горизонтальном положении и 10 изображений каждой цифры в вертикальном положении,\n",
    "\n",
    "2. Создать новые тестовые картинки, полученные путем добавления черных пикселей (шум) в случайно выбранные места сгенерированных тестовых картинок:\n",
    "* 20 пикселей (тестовая выборка № 2),\n",
    "* 50 пикселей (тестовая выборка № 3), \n",
    "* 100 пикселей (тестовая выборка № 4),\n",
    "* 200 пикселей (тестовая выборка № 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59f66938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93e7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Генерация изображений\n",
    "def generate_images():\n",
    "    # Параметры изображения\n",
    "    image_size = (100, 100)\n",
    "    digit_size = (20, 50)\n",
    "    background_color = (255, 255, 255)  # белый\n",
    "    digit_color = (0, 0, 0)             # черный\n",
    "    digits = ['0', '1', '3', '8']\n",
    "\n",
    "    # Создание папок\n",
    "    def create_folders():\n",
    "        folders = [\n",
    "            'train/horizontal', 'train/vertical',\n",
    "            'test1/horizontal', 'test1/vertical',\n",
    "            'test2/horizontal', 'test2/vertical',\n",
    "            'test3/horizontal', 'test3/vertical',\n",
    "            'test4/horizontal', 'test4/vertical',\n",
    "            'test5/horizontal', 'test5/vertical'\n",
    "        ]\n",
    "        for folder in folders:\n",
    "            for digit in digits:\n",
    "                os.makedirs(os.path.join(folder, digit), exist_ok=True)\n",
    "\n",
    "    # Генерация одной цифры\n",
    "    def generate_digit(digit, orientation, path, count):\n",
    "        img = Image.new('RGB', image_size, background_color)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", size=max(digit_size))\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "            font_size = 1\n",
    "            while True:\n",
    "                test_font = ImageFont.load_default()\n",
    "                left, top, right, bottom = draw.textbbox((0, 0), digit, font=test_font)\n",
    "                w, h = right - left, bottom - top\n",
    "                if w > digit_size[0] or h > digit_size[1]:\n",
    "                    break\n",
    "                font = test_font\n",
    "                font_size += 1\n",
    "        \n",
    "        left, top, right, bottom = draw.textbbox((0, 0), digit, font=font)\n",
    "        digit_width, digit_height = right - left, bottom - top\n",
    "        \n",
    "        if orientation == 'horizontal':\n",
    "            max_x = image_size[0] - digit_width\n",
    "            max_y = image_size[1] - digit_height\n",
    "            pos_x = random.randint(0, max_x)\n",
    "            pos_y = random.randint(0, max_y)\n",
    "            position = (pos_x, pos_y)\n",
    "        else:\n",
    "            img = img.rotate(90, expand=True)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            max_x = image_size[1] - digit_width\n",
    "            max_y = image_size[0] - digit_height\n",
    "            pos_x = random.randint(0, max_x)\n",
    "            pos_y = random.randint(0, max_y)\n",
    "            position = (pos_x, pos_y)\n",
    "        \n",
    "        draw.text(position, digit, fill=digit_color, font=font)\n",
    "        \n",
    "        if orientation == 'vertical':\n",
    "            img = img.rotate(-90, expand=True)\n",
    "        \n",
    "        img.save(os.path.join(path, orientation, digit, f'{digit}_{count}.png'))\n",
    "\n",
    "    # Добавление шума\n",
    "    def add_noise(image_path, output_path, noise_pixels):\n",
    "        img = Image.open(image_path)\n",
    "        pixels = img.load()\n",
    "        width, height = img.size\n",
    "        \n",
    "        for _ in range(noise_pixels):\n",
    "            x = random.randint(0, width - 1)\n",
    "            y = random.randint(0, height - 1)\n",
    "            pixels[x, y] = digit_color\n",
    "        \n",
    "        img.save(output_path)\n",
    "\n",
    "    # Основная генерация\n",
    "    create_folders()\n",
    "    \n",
    "    # Тренировочные данные (800 на цифру)\n",
    "    for digit in digits:\n",
    "        for i in range(400):\n",
    "            generate_digit(digit, 'horizontal', 'train', i)\n",
    "            generate_digit(digit, 'vertical', 'train', i)\n",
    "    \n",
    "    # Тестовые данные (20 на цифру)\n",
    "    for digit in digits:\n",
    "        for i in range(10):\n",
    "            generate_digit(digit, 'horizontal', 'test1', i)\n",
    "            generate_digit(digit, 'vertical', 'test1', i)\n",
    "    \n",
    "    # Тестовые выборки с шумом\n",
    "    for digit in digits:\n",
    "        for orientation in ['horizontal', 'vertical']:\n",
    "            for i in range(10):\n",
    "                input_path = os.path.join('test1', orientation, digit, f'{digit}_{i}.png')\n",
    "                \n",
    "                for noise, test_num in [(20, 'test2'), (50, 'test3'), (100, 'test4'), (200, 'test5')]:\n",
    "                    output_path = os.path.join(test_num, orientation, digit, f'{digit}_{i}.png')\n",
    "                    add_noise(input_path, output_path, noise)\n",
    "\n",
    "\n",
    "generate_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3360dc",
   "metadata": {},
   "source": [
    "# Задание №2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0459a",
   "metadata": {},
   "source": [
    "Не используя предобученные модели (сети), модифицировать скрипт задачи «Dogs vs Cats» (с семинара) или написать свою нейронную сеть на keras такую, что:\n",
    "\n",
    "1) На вход подается тренировочное множество: по 800 картинок каждой цифры.\n",
    "2) Из тренировочного множества выделяется часть картинок (10-20%), на валидационное множество, в котором должны присутствовать цифры в вертикальном и горизонтальном положении.\n",
    "3) Протестировать адекватность модели на всех тестовых выборках № 1, № 2, № 3, № 4, № 5, фиксируя при этом точность (accuracy) классификации.\n",
    "4) Повторить пункты 1)–3), изменив объем тренировочной выборки до 600, 400, 200, 100 картинок каждой цифры.\n",
    "\n",
    "<i> Могут пригодиться <tt>Dense</tt>, <tt>Conv2D</tt>, <tt>MaxPooling2D</tt>, <tt>Flatten</tt>.</i>\n",
    "\n",
    "Результаты оформить в виде таблицы со столбцами: размер тренировочной выборки, количество шумовых пикселей, точность (accuracy) классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb46050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments():\n",
    "    # Основные параметры данных\n",
    "    digits = ['0', '1', '3', '8']  # Цифры для классификации\n",
    "    image_size = (100, 100)        # Размер изображений\n",
    "    input_shape = (100, 100, 1)    # Формат входных данных для сети (с каналом grayscale)\n",
    "    num_classes = 4                # Количество классов классификации\n",
    "    epochs = 15                    # Количество эпох обучения\n",
    "    batch_size = 32                # Размер батча\n",
    "\n",
    "    # Загрузка изображений\n",
    "    def load_images(folder, digit, orientation):\n",
    "        images = []\n",
    "        labels = []\n",
    "        path = os.path.join(folder, orientation, digit)\n",
    "        \n",
    "        # Проверка существования папки\n",
    "        if not os.path.exists(path):\n",
    "            return [], []\n",
    "            \n",
    "        # Чтение всех изображений в папке\n",
    "        for img_name in os.listdir(path):\n",
    "            img_path = os.path.join(path, img_name)\n",
    "            img = Image.open(img_path).convert('L')  # Конвертация в grayscale\n",
    "            img = np.array(img) / 255.0              # Нормализация [0, 1]\n",
    "            images.append(img)\n",
    "            labels.append(digits.index(digit))       # Метка как индекс цифры\n",
    "        return images, labels\n",
    "\n",
    "    # Загрузка всех данных\n",
    "    def load_all_data(folder):\n",
    "        all_images = []\n",
    "        all_labels = []\n",
    "        # Для каждой цифры и ориентации загружаем изображения\n",
    "        for digit in digits:\n",
    "            for orientation in ['horizontal', 'vertical']:\n",
    "                images, labels = load_images(folder, digit, orientation)\n",
    "                all_images.extend(images)\n",
    "                all_labels.extend(labels)\n",
    "        return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "    # Создание модели\n",
    "    def create_model():\n",
    "        model = Sequential([\n",
    "            # Первый сверточный слой\n",
    "            Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            # Второй сверточный слой\n",
    "            Conv2D(64, (3, 3), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            # Преобразование в 1D для полносвязных слоев\n",
    "            Flatten(),\n",
    "            # Полносвязный слой\n",
    "            Dense(128, activation='relu'),\n",
    "            # Выходной слой с softmax для классификации\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Компиляция модели\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                     optimizer='adam', \n",
    "                     metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    # Основной эксперимент\n",
    "    results = []  # Для хранения результатов\n",
    "    train_sizes = [800, 600, 400, 200, 100]  # Размеры обучающих выборок\n",
    "    \n",
    "    for train_size in train_sizes:\n",
    "        # Загрузка полного тренировочного набора\n",
    "        X_train_full, y_train_full = load_all_data('train')\n",
    "        \n",
    "        # Уменьшение выборки если требуется\n",
    "        if train_size < 800:\n",
    "            # Стратифицированное разбиение для сохранения баланса классов\n",
    "            X_train, _, y_train, _ = train_test_split(\n",
    "                X_train_full, y_train_full, \n",
    "                train_size=train_size*4,  # 4 класса цифр\n",
    "                stratify=y_train_full)\n",
    "        else:\n",
    "            X_train, y_train = X_train_full, y_train_full\n",
    "        \n",
    "        # Разделение на тренировочную и валидационную выборки (15% валидации)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, \n",
    "            test_size=0.15, \n",
    "            stratify=y_train)\n",
    "        \n",
    "        # Преобразование меток в one-hot encoding\n",
    "        y_train = to_categorical(y_train, num_classes)\n",
    "        y_val = to_categorical(y_val, num_classes)\n",
    "        \n",
    "        # Создание и обучение модели\n",
    "        model = create_model()\n",
    "        model.fit(\n",
    "            X_train.reshape(-1, *input_shape),  # Изменение формы данных\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            # verbose=0,  # Без вывода процесса обучения\n",
    "            validation_data=(X_val.reshape(-1, *input_shape), y_val))\n",
    "        \n",
    "        # Тестирование на всех вариантах шума\n",
    "        for noise_pixels, test_num in [(0, 'test1'), (20, 'test2'), \n",
    "                                     (50, 'test3'), (100, 'test4'), \n",
    "                                     (200, 'test5')]:\n",
    "            X_test, y_test = load_all_data(test_num)\n",
    "            if len(X_test) == 0:  # Пропуск если нет данных\n",
    "                continue\n",
    "                \n",
    "            y_test = to_categorical(y_test, num_classes)\n",
    "            # Оценка точности на тестовых данных\n",
    "            loss, accuracy = model.evaluate(\n",
    "                X_test.reshape(-1, *input_shape), \n",
    "                y_test, \n",
    "                verbose=0)\n",
    "            \n",
    "            # Сохранение результатов\n",
    "            results.append({\n",
    "                'Train size': train_size,        # Размер обучающей выборки\n",
    "                'Noise pixels': noise_pixels,    # Количество шумовых пикселей\n",
    "                'Accuracy': accuracy             # Точность классификации\n",
    "            })\n",
    "    \n",
    "    # Анализ и сохранение результатов\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nРезультаты:\")\n",
    "    # Сводная таблица точности по разным условиям\n",
    "    print(df.pivot_table(index='Train size', \n",
    "                        columns='Noise pixels', \n",
    "                        values='Accuracy'))\n",
    "    \n",
    "    # Сохранение в CSV файл\n",
    "    df.to_csv('experiment_results.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beed2164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\murza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - accuracy: 0.3315 - loss: 1.8122 - val_accuracy: 0.6354 - val_loss: 0.8423\n",
      "Epoch 2/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 82ms/step - accuracy: 0.6964 - loss: 0.7251 - val_accuracy: 0.7271 - val_loss: 0.6484\n",
      "Epoch 3/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step - accuracy: 0.8197 - loss: 0.4880 - val_accuracy: 0.7479 - val_loss: 0.5307\n",
      "Epoch 4/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 81ms/step - accuracy: 0.8900 - loss: 0.3202 - val_accuracy: 0.8125 - val_loss: 0.4299\n",
      "Epoch 5/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 82ms/step - accuracy: 0.9562 - loss: 0.1888 - val_accuracy: 0.9167 - val_loss: 0.2393\n",
      "Epoch 6/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.9831 - loss: 0.0969 - val_accuracy: 0.9417 - val_loss: 0.1648\n",
      "Epoch 7/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9957 - loss: 0.0445 - val_accuracy: 0.9729 - val_loss: 0.0907\n",
      "Epoch 8/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - accuracy: 0.9942 - loss: 0.0338 - val_accuracy: 0.9812 - val_loss: 0.0740\n",
      "Epoch 9/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0100 - val_accuracy: 0.9812 - val_loss: 0.0636\n",
      "Epoch 10/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.9833 - val_loss: 0.0595\n",
      "Epoch 11/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.9812 - val_loss: 0.0608\n",
      "Epoch 12/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.9854 - val_loss: 0.0575\n",
      "Epoch 13/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.9812 - val_loss: 0.0520\n",
      "Epoch 14/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9812 - val_loss: 0.0553\n",
      "Epoch 15/15\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9812 - val_loss: 0.0520\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\murza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.2651 - loss: 1.6025 - val_accuracy: 0.5861 - val_loss: 0.9297\n",
      "Epoch 2/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step - accuracy: 0.6517 - loss: 0.7778 - val_accuracy: 0.6694 - val_loss: 0.6917\n",
      "Epoch 3/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - accuracy: 0.7658 - loss: 0.5706 - val_accuracy: 0.7639 - val_loss: 0.5713\n",
      "Epoch 4/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 98ms/step - accuracy: 0.8417 - loss: 0.4083 - val_accuracy: 0.7889 - val_loss: 0.4926\n",
      "Epoch 5/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 89ms/step - accuracy: 0.8996 - loss: 0.3025 - val_accuracy: 0.7194 - val_loss: 0.5848\n",
      "Epoch 6/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 0.9240 - loss: 0.2318 - val_accuracy: 0.8444 - val_loss: 0.3793\n",
      "Epoch 7/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 95ms/step - accuracy: 0.9617 - loss: 0.1567 - val_accuracy: 0.8917 - val_loss: 0.3036\n",
      "Epoch 8/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step - accuracy: 0.9757 - loss: 0.1171 - val_accuracy: 0.8806 - val_loss: 0.2963\n",
      "Epoch 9/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step - accuracy: 0.9790 - loss: 0.0896 - val_accuracy: 0.9028 - val_loss: 0.2683\n",
      "Epoch 10/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 79ms/step - accuracy: 0.9997 - loss: 0.0387 - val_accuracy: 0.9056 - val_loss: 0.2473\n",
      "Epoch 11/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0178 - val_accuracy: 0.9194 - val_loss: 0.2561\n",
      "Epoch 12/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0124 - val_accuracy: 0.9194 - val_loss: 0.2363\n",
      "Epoch 13/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0090 - val_accuracy: 0.9306 - val_loss: 0.2371\n",
      "Epoch 14/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.9278 - val_loss: 0.2451\n",
      "Epoch 15/15\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9250 - val_loss: 0.2538\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\murza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 82ms/step - accuracy: 0.2660 - loss: 1.7124 - val_accuracy: 0.2542 - val_loss: 1.3583\n",
      "Epoch 2/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.3443 - loss: 1.2916 - val_accuracy: 0.3750 - val_loss: 1.2839\n",
      "Epoch 3/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.6313 - loss: 0.9500 - val_accuracy: 0.6167 - val_loss: 0.9878\n",
      "Epoch 4/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - accuracy: 0.8135 - loss: 0.5037 - val_accuracy: 0.6708 - val_loss: 0.8702\n",
      "Epoch 5/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.9298 - loss: 0.2160 - val_accuracy: 0.6875 - val_loss: 0.9762\n",
      "Epoch 6/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.9745 - loss: 0.0933 - val_accuracy: 0.7042 - val_loss: 0.9655\n",
      "Epoch 7/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 82ms/step - accuracy: 0.9890 - loss: 0.0524 - val_accuracy: 0.6958 - val_loss: 0.9792\n",
      "Epoch 8/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.9974 - loss: 0.0224 - val_accuracy: 0.7667 - val_loss: 0.8779\n",
      "Epoch 9/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.0083 - val_accuracy: 0.7625 - val_loss: 0.9181\n",
      "Epoch 10/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.9986 - loss: 0.0062 - val_accuracy: 0.7417 - val_loss: 1.0484\n",
      "Epoch 11/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.9975 - loss: 0.0131 - val_accuracy: 0.7500 - val_loss: 0.9726\n",
      "Epoch 12/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - accuracy: 0.9990 - loss: 0.0073 - val_accuracy: 0.7500 - val_loss: 0.9532\n",
      "Epoch 13/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7667 - val_loss: 0.9620\n",
      "Epoch 14/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 9.7381e-04 - val_accuracy: 0.7792 - val_loss: 0.9818\n",
      "Epoch 15/15\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 7.7684e-04 - val_accuracy: 0.7792 - val_loss: 1.0067\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\murza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.2404 - loss: 2.0912 - val_accuracy: 0.2500 - val_loss: 1.3669\n",
      "Epoch 2/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.2871 - loss: 1.3358 - val_accuracy: 0.2500 - val_loss: 1.3433\n",
      "Epoch 3/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.4726 - loss: 1.1120 - val_accuracy: 0.3167 - val_loss: 1.4342\n",
      "Epoch 4/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7544 - loss: 0.7329 - val_accuracy: 0.4333 - val_loss: 1.4823\n",
      "Epoch 5/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.8976 - loss: 0.4055 - val_accuracy: 0.5000 - val_loss: 1.8461\n",
      "Epoch 6/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.9377 - loss: 0.1886 - val_accuracy: 0.4333 - val_loss: 2.1826\n",
      "Epoch 7/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.9814 - loss: 0.1111 - val_accuracy: 0.4667 - val_loss: 2.3195\n",
      "Epoch 8/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9935 - loss: 0.0611 - val_accuracy: 0.4833 - val_loss: 2.3414\n",
      "Epoch 9/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9975 - loss: 0.0282 - val_accuracy: 0.5167 - val_loss: 2.4582\n",
      "Epoch 10/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9995 - loss: 0.0135 - val_accuracy: 0.5083 - val_loss: 2.5844\n",
      "Epoch 11/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 0.5083 - val_loss: 2.6813\n",
      "Epoch 12/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.5250 - val_loss: 2.8212\n",
      "Epoch 13/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.5167 - val_loss: 2.8999\n",
      "Epoch 14/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.5333 - val_loss: 2.9886\n",
      "Epoch 15/15\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.5417 - val_loss: 3.0395\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\murza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.2720 - loss: 3.1481 - val_accuracy: 0.2833 - val_loss: 1.3887\n",
      "Epoch 2/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - accuracy: 0.3055 - loss: 1.3773 - val_accuracy: 0.2500 - val_loss: 1.3809\n",
      "Epoch 3/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - accuracy: 0.3797 - loss: 1.3549 - val_accuracy: 0.2667 - val_loss: 1.3588\n",
      "Epoch 4/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.4923 - loss: 1.2703 - val_accuracy: 0.3833 - val_loss: 1.3246\n",
      "Epoch 5/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.7095 - loss: 1.0222 - val_accuracy: 0.4500 - val_loss: 1.2643\n",
      "Epoch 6/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 0.8115 - loss: 0.7140 - val_accuracy: 0.4333 - val_loss: 1.3558\n",
      "Epoch 7/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - accuracy: 0.8865 - loss: 0.4375 - val_accuracy: 0.5000 - val_loss: 1.3322\n",
      "Epoch 8/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - accuracy: 0.9317 - loss: 0.2659 - val_accuracy: 0.4000 - val_loss: 1.6181\n",
      "Epoch 9/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.9354 - loss: 0.2553 - val_accuracy: 0.5333 - val_loss: 1.7235\n",
      "Epoch 10/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.9885 - loss: 0.1092 - val_accuracy: 0.5333 - val_loss: 1.7856\n",
      "Epoch 11/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.9822 - loss: 0.1048 - val_accuracy: 0.5000 - val_loss: 1.9386\n",
      "Epoch 12/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 0.0616 - val_accuracy: 0.5667 - val_loss: 1.9405\n",
      "Epoch 13/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 0.0221 - val_accuracy: 0.5667 - val_loss: 2.0588\n",
      "Epoch 14/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.0169 - val_accuracy: 0.5333 - val_loss: 2.2111\n",
      "Epoch 15/15\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 0.0118 - val_accuracy: 0.5833 - val_loss: 2.3116\n",
      "\n",
      "Результаты эксперимента:\n",
      "Noise pixels     0       20      50      100     200\n",
      "Train size                                          \n",
      "100           0.5750  0.4625  0.3625  0.2375  0.2500\n",
      "200           0.5500  0.5500  0.5500  0.4875  0.4625\n",
      "400           0.7625  0.7625  0.7375  0.7500  0.5625\n",
      "600           0.9000  0.8625  0.8375  0.7250  0.3125\n",
      "800           0.9625  0.9625  0.9375  0.9375  0.5000\n"
     ]
    }
   ],
   "source": [
    "results = run_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
